{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Higher school of econimics\n",
        "#### Name: Zhornichenko Ilya\n",
        "#### Group: TMSS \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# when running in colab, un-comment this\n",
        "!wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall19/week03_convnets/cifar.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y4c-Avy_t3A",
        "outputId": "8adf38c3-df20-4804-9e1b-eb34553b3563"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset not found. Downloading...\n",
            "(40000, 3, 32, 32) (40000,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from cifar import load_cifar10\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10(\"cifar_data\")\n",
        "\n",
        "class_names = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                        'dog', 'frog', 'horse', 'ship', 'truck'])\n",
        "\n",
        "print(X_train.shape,y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tTZMtBwfAEJ8"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms as T, models\n",
        "from torch import optim\n",
        "from torchvision import models\n",
        "import torch\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "import seaborn as sns\n",
        "from torchvision import transforms, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YAoAIu8qALQj"
      },
      "outputs": [],
      "source": [
        "n_epochs = 100\n",
        "batch_size_train = 32\n",
        "batch_size_test = 32\n",
        "lr = 0.001\n",
        "momentum = 0.5\n",
        "log_interval = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ml2V-UOEAkZz"
      },
      "outputs": [],
      "source": [
        "class DataSet(Dataset):\n",
        "  def __init__(self, images, labels):\n",
        "    self.images = images\n",
        "    self.labels = labels\n",
        "    self.my_transforms = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img = self.my_transforms(np.transpose(self.images[idx], (1, 2, 0)))\n",
        "    return img, self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c2NDnEQ0AQPE"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(DataSet(X_train, y_train), batch_size=batch_size_train , shuffle=True, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(DataSet(X_test, y_test), batch_size=batch_size_test , shuffle=True, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rpwrL7GeBYdX"
      },
      "outputs": [],
      "source": [
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_LGUiqvO0Ws",
        "outputId": "597b7e9f-33e7-4178-9ff1-4ea8151446e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 32, 32])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QiIOnLlcMCVq"
      },
      "outputs": [],
      "source": [
        "dataloaders = {\n",
        "    'train':train_loader, \n",
        "    'val': test_loader\n",
        "}\n",
        "dataset_sizes = {\n",
        "    'train':X_train.shape[0],\n",
        "    'val':X_test.shape[0]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5wmNwuN6MSyH"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(8, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eqcLJ1AuNERg"
      },
      "outputs": [],
      "source": [
        "net = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oyyNA9QHNEXf"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "cpu = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_-FQCPZ2NGQE"
      },
      "outputs": [],
      "source": [
        "net = net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "W6jEpVzwMjnO"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Fdh4I3fyMlwz"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device).long()\n",
        "                # print(inputs.shape)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hnGMV_nvM4Mj",
        "outputId": "b6b5e450-d26c-47b8-e1f4-42434338118a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 2.3024 Acc: 0.1101\n",
            "val Loss: 2.3009 Acc: 0.1573\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 2.2932 Acc: 0.1521\n",
            "val Loss: 2.2603 Acc: 0.1329\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 2.1171 Acc: 0.2052\n",
            "val Loss: 1.9971 Acc: 0.2583\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 1.9558 Acc: 0.2792\n",
            "val Loss: 1.8473 Acc: 0.3221\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 1.7807 Acc: 0.3543\n",
            "val Loss: 1.7003 Acc: 0.3784\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 1.6487 Acc: 0.4010\n",
            "val Loss: 1.5634 Acc: 0.4311\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 1.5516 Acc: 0.4400\n",
            "val Loss: 1.5158 Acc: 0.4461\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 1.4917 Acc: 0.4615\n",
            "val Loss: 1.4540 Acc: 0.4655\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 1.4386 Acc: 0.4825\n",
            "val Loss: 1.4169 Acc: 0.4839\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 1.3960 Acc: 0.4949\n",
            "val Loss: 1.3940 Acc: 0.4929\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 1.3604 Acc: 0.5124\n",
            "val Loss: 1.3605 Acc: 0.5054\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 1.3310 Acc: 0.5234\n",
            "val Loss: 1.3320 Acc: 0.5172\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 1.2951 Acc: 0.5373\n",
            "val Loss: 1.3124 Acc: 0.5315\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 1.2663 Acc: 0.5479\n",
            "val Loss: 1.2711 Acc: 0.5486\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 1.2314 Acc: 0.5647\n",
            "val Loss: 1.2895 Acc: 0.5439\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 1.2032 Acc: 0.5748\n",
            "val Loss: 1.2250 Acc: 0.5651\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 1.1759 Acc: 0.5849\n",
            "val Loss: 1.2273 Acc: 0.5629\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 1.1423 Acc: 0.5962\n",
            "val Loss: 1.2324 Acc: 0.5650\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 1.1203 Acc: 0.6055\n",
            "val Loss: 1.2021 Acc: 0.5827\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 1.0888 Acc: 0.6149\n",
            "val Loss: 1.1912 Acc: 0.5784\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 1.0663 Acc: 0.6249\n",
            "val Loss: 1.1802 Acc: 0.5912\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 1.0392 Acc: 0.6350\n",
            "val Loss: 1.1571 Acc: 0.5943\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 1.0173 Acc: 0.6428\n",
            "val Loss: 1.1380 Acc: 0.6028\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 0.9895 Acc: 0.6544\n",
            "val Loss: 1.1252 Acc: 0.6085\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 0.9693 Acc: 0.6611\n",
            "val Loss: 1.1491 Acc: 0.6052\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 0.9458 Acc: 0.6712\n",
            "val Loss: 1.1271 Acc: 0.6108\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 0.9244 Acc: 0.6745\n",
            "val Loss: 1.1599 Acc: 0.6005\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 0.9021 Acc: 0.6854\n",
            "val Loss: 1.1444 Acc: 0.6114\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 0.8806 Acc: 0.6904\n",
            "val Loss: 1.1366 Acc: 0.6139\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 0.8630 Acc: 0.6992\n",
            "val Loss: 1.1213 Acc: 0.6180\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 0.8417 Acc: 0.7044\n",
            "val Loss: 1.1302 Acc: 0.6156\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 0.8246 Acc: 0.7114\n",
            "val Loss: 1.1776 Acc: 0.5993\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 0.8065 Acc: 0.7177\n",
            "val Loss: 1.1456 Acc: 0.6115\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 0.7847 Acc: 0.7233\n",
            "val Loss: 1.1277 Acc: 0.6261\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 0.7616 Acc: 0.7322\n",
            "val Loss: 1.1641 Acc: 0.6145\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 0.7477 Acc: 0.7374\n",
            "val Loss: 1.1413 Acc: 0.6217\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 0.7326 Acc: 0.7415\n",
            "val Loss: 1.1612 Acc: 0.6200\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 0.7162 Acc: 0.7477\n",
            "val Loss: 1.1746 Acc: 0.6229\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 0.6990 Acc: 0.7535\n",
            "val Loss: 1.1932 Acc: 0.6109\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 0.6827 Acc: 0.7602\n",
            "val Loss: 1.2542 Acc: 0.6087\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 0.6651 Acc: 0.7660\n",
            "val Loss: 1.2040 Acc: 0.6219\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 0.6514 Acc: 0.7704\n",
            "val Loss: 1.1807 Acc: 0.6207\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 0.6347 Acc: 0.7745\n",
            "val Loss: 1.2094 Acc: 0.6209\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 0.6137 Acc: 0.7831\n",
            "val Loss: 1.2622 Acc: 0.6136\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 0.6013 Acc: 0.7875\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-cebf8dd6a592>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-aa3894ee8b73>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mforeach\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                                 \u001b[0mper_device_and_dtype_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_model(net, criterion, optimizer,  num_epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFZpNtRHMH8B"
      },
      "outputs": [],
      "source": [
        "class Net2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(8, 16, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Conv2d(16, 32, 3)\n",
        "        self.fc1 = nn.Linear(128, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKIneMarRSDe"
      },
      "outputs": [],
      "source": [
        "net2 = Net2()\n",
        "net2 = net2.to(device)\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "optimizer2 = optim.SGD(net2.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fB3_1DXRc8b",
        "outputId": "083eb5af-22d2-422a-c137-9d3333e302a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 2.3030 Acc: 0.1019\n",
            "val Loss: 2.3023 Acc: 0.1000\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 2.3022 Acc: 0.1058\n",
            "val Loss: 2.3019 Acc: 0.1268\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 2.3015 Acc: 0.1212\n",
            "val Loss: 2.3007 Acc: 0.1500\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 2.2990 Acc: 0.1322\n",
            "val Loss: 2.2949 Acc: 0.1402\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 2.2638 Acc: 0.1515\n",
            "val Loss: 2.1710 Acc: 0.1758\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 2.0987 Acc: 0.1977\n",
            "val Loss: 2.0438 Acc: 0.2267\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 2.0157 Acc: 0.2279\n",
            "val Loss: 1.9725 Acc: 0.2334\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 1.9398 Acc: 0.2527\n",
            "val Loss: 1.8837 Acc: 0.2626\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 1.8504 Acc: 0.2953\n",
            "val Loss: 1.7986 Acc: 0.3173\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 1.7588 Acc: 0.3468\n",
            "val Loss: 1.6922 Acc: 0.3753\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 1.6785 Acc: 0.3860\n",
            "val Loss: 1.6324 Acc: 0.4003\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 1.6215 Acc: 0.4072\n",
            "val Loss: 1.5693 Acc: 0.4260\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 1.5728 Acc: 0.4249\n",
            "val Loss: 1.5480 Acc: 0.4373\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 1.5290 Acc: 0.4423\n",
            "val Loss: 1.4980 Acc: 0.4569\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 1.4974 Acc: 0.4551\n",
            "val Loss: 1.4693 Acc: 0.4646\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 1.4699 Acc: 0.4642\n",
            "val Loss: 1.4448 Acc: 0.4780\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 1.4375 Acc: 0.4798\n",
            "val Loss: 1.4214 Acc: 0.4837\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 1.4114 Acc: 0.4864\n",
            "val Loss: 1.3889 Acc: 0.4961\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 1.3766 Acc: 0.5011\n",
            "val Loss: 1.3515 Acc: 0.5098\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 1.3518 Acc: 0.5132\n",
            "val Loss: 1.3693 Acc: 0.5112\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 1.3212 Acc: 0.5239\n",
            "val Loss: 1.3116 Acc: 0.5272\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 1.2906 Acc: 0.5339\n",
            "val Loss: 1.2805 Acc: 0.5433\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 1.2650 Acc: 0.5483\n",
            "val Loss: 1.2823 Acc: 0.5394\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 1.2348 Acc: 0.5569\n",
            "val Loss: 1.2498 Acc: 0.5516\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 1.2141 Acc: 0.5647\n",
            "val Loss: 1.2129 Acc: 0.5720\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 1.1864 Acc: 0.5776\n",
            "val Loss: 1.2152 Acc: 0.5679\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 1.1659 Acc: 0.5848\n",
            "val Loss: 1.1965 Acc: 0.5773\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 1.1450 Acc: 0.5927\n",
            "val Loss: 1.1664 Acc: 0.5838\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 1.1264 Acc: 0.5991\n",
            "val Loss: 1.1527 Acc: 0.5876\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 1.1073 Acc: 0.6058\n",
            "val Loss: 1.1865 Acc: 0.5779\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 1.0905 Acc: 0.6110\n",
            "val Loss: 1.1421 Acc: 0.5957\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 1.0741 Acc: 0.6190\n",
            "val Loss: 1.1408 Acc: 0.5923\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 1.0561 Acc: 0.6241\n",
            "val Loss: 1.1198 Acc: 0.6044\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 1.0405 Acc: 0.6304\n",
            "val Loss: 1.1095 Acc: 0.6053\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 1.0226 Acc: 0.6363\n",
            "val Loss: 1.1310 Acc: 0.5965\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 1.0064 Acc: 0.6436\n",
            "val Loss: 1.0856 Acc: 0.6149\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 0.9933 Acc: 0.6468\n",
            "val Loss: 1.1148 Acc: 0.6048\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 0.9796 Acc: 0.6544\n",
            "val Loss: 1.1141 Acc: 0.6100\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 0.9632 Acc: 0.6621\n",
            "val Loss: 1.0846 Acc: 0.6170\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 0.9469 Acc: 0.6661\n",
            "val Loss: 1.0901 Acc: 0.6188\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 0.9360 Acc: 0.6694\n",
            "val Loss: 1.0684 Acc: 0.6274\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 0.9256 Acc: 0.6733\n",
            "val Loss: 1.0474 Acc: 0.6321\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 0.9107 Acc: 0.6801\n",
            "val Loss: 1.0534 Acc: 0.6307\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 0.9033 Acc: 0.6790\n",
            "val Loss: 1.0514 Acc: 0.6341\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 0.8913 Acc: 0.6869\n",
            "val Loss: 1.0503 Acc: 0.6365\n",
            "\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 0.8759 Acc: 0.6921\n",
            "val Loss: 1.0758 Acc: 0.6267\n",
            "\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 0.8684 Acc: 0.6937\n",
            "val Loss: 1.0368 Acc: 0.6352\n",
            "\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 0.8574 Acc: 0.6981\n",
            "val Loss: 1.0554 Acc: 0.6374\n",
            "\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 0.8467 Acc: 0.7016\n",
            "val Loss: 1.0341 Acc: 0.6403\n",
            "\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 0.8386 Acc: 0.7041\n",
            "val Loss: 1.0580 Acc: 0.6370\n",
            "\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 0.8274 Acc: 0.7088\n",
            "val Loss: 1.0397 Acc: 0.6407\n",
            "\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 0.8193 Acc: 0.7126\n",
            "val Loss: 1.0591 Acc: 0.6347\n",
            "\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 0.8075 Acc: 0.7154\n",
            "val Loss: 1.0267 Acc: 0.6493\n",
            "\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 0.7990 Acc: 0.7185\n",
            "val Loss: 1.0456 Acc: 0.6415\n",
            "\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 0.7883 Acc: 0.7238\n",
            "val Loss: 1.0346 Acc: 0.6456\n",
            "\n",
            "Epoch 55/99\n",
            "----------\n",
            "train Loss: 0.7834 Acc: 0.7247\n",
            "val Loss: 1.0378 Acc: 0.6464\n",
            "\n",
            "Epoch 56/99\n",
            "----------\n",
            "train Loss: 0.7732 Acc: 0.7284\n",
            "val Loss: 1.0799 Acc: 0.6323\n",
            "\n",
            "Epoch 57/99\n",
            "----------\n",
            "train Loss: 0.7639 Acc: 0.7312\n",
            "val Loss: 1.0526 Acc: 0.6458\n",
            "\n",
            "Epoch 58/99\n",
            "----------\n",
            "train Loss: 0.7577 Acc: 0.7319\n",
            "val Loss: 1.0441 Acc: 0.6462\n",
            "\n",
            "Epoch 59/99\n",
            "----------\n",
            "train Loss: 0.7466 Acc: 0.7364\n",
            "val Loss: 1.0766 Acc: 0.6313\n",
            "\n",
            "Epoch 60/99\n",
            "----------\n",
            "train Loss: 0.7374 Acc: 0.7410\n",
            "val Loss: 1.0474 Acc: 0.6500\n",
            "\n",
            "Epoch 61/99\n",
            "----------\n",
            "train Loss: 0.7317 Acc: 0.7419\n",
            "val Loss: 1.0525 Acc: 0.6512\n",
            "\n",
            "Epoch 62/99\n",
            "----------\n",
            "train Loss: 0.7211 Acc: 0.7459\n",
            "val Loss: 1.0743 Acc: 0.6392\n",
            "\n",
            "Epoch 63/99\n",
            "----------\n",
            "train Loss: 0.7149 Acc: 0.7474\n",
            "val Loss: 1.1077 Acc: 0.6405\n",
            "\n",
            "Epoch 64/99\n",
            "----------\n",
            "train Loss: 0.7069 Acc: 0.7487\n",
            "val Loss: 1.0762 Acc: 0.6466\n",
            "\n",
            "Epoch 65/99\n",
            "----------\n",
            "train Loss: 0.6956 Acc: 0.7550\n",
            "val Loss: 1.0759 Acc: 0.6385\n",
            "\n",
            "Epoch 66/99\n",
            "----------\n",
            "train Loss: 0.6952 Acc: 0.7563\n",
            "val Loss: 1.0958 Acc: 0.6404\n",
            "\n",
            "Epoch 67/99\n",
            "----------\n",
            "train Loss: 0.6841 Acc: 0.7564\n",
            "val Loss: 1.1091 Acc: 0.6429\n",
            "\n",
            "Epoch 68/99\n",
            "----------\n",
            "train Loss: 0.6789 Acc: 0.7580\n",
            "val Loss: 1.1107 Acc: 0.6448\n",
            "\n",
            "Epoch 69/99\n",
            "----------\n",
            "train Loss: 0.6648 Acc: 0.7653\n",
            "val Loss: 1.1022 Acc: 0.6469\n",
            "\n",
            "Epoch 70/99\n",
            "----------\n",
            "train Loss: 0.6642 Acc: 0.7650\n",
            "val Loss: 1.1177 Acc: 0.6414\n",
            "\n",
            "Epoch 71/99\n",
            "----------\n",
            "train Loss: 0.6565 Acc: 0.7685\n",
            "val Loss: 1.1139 Acc: 0.6385\n",
            "\n",
            "Epoch 72/99\n",
            "----------\n",
            "train Loss: 0.6449 Acc: 0.7732\n",
            "val Loss: 1.1642 Acc: 0.6345\n",
            "\n",
            "Epoch 73/99\n",
            "----------\n",
            "train Loss: 0.6432 Acc: 0.7753\n",
            "val Loss: 1.1081 Acc: 0.6472\n",
            "\n",
            "Epoch 74/99\n",
            "----------\n",
            "train Loss: 0.6364 Acc: 0.7760\n",
            "val Loss: 1.1075 Acc: 0.6492\n",
            "\n",
            "Epoch 75/99\n",
            "----------\n",
            "train Loss: 0.6269 Acc: 0.7788\n",
            "val Loss: 1.1023 Acc: 0.6520\n",
            "\n",
            "Epoch 76/99\n",
            "----------\n",
            "train Loss: 0.6184 Acc: 0.7843\n",
            "val Loss: 1.1540 Acc: 0.6306\n",
            "\n",
            "Epoch 77/99\n",
            "----------\n",
            "train Loss: 0.6067 Acc: 0.7855\n",
            "val Loss: 1.1671 Acc: 0.6415\n",
            "\n",
            "Epoch 78/99\n",
            "----------\n",
            "train Loss: 0.6018 Acc: 0.7861\n",
            "val Loss: 1.1300 Acc: 0.6424\n",
            "\n",
            "Epoch 79/99\n",
            "----------\n",
            "train Loss: 0.5983 Acc: 0.7886\n",
            "val Loss: 1.1980 Acc: 0.6443\n",
            "\n",
            "Epoch 80/99\n",
            "----------\n",
            "train Loss: 0.5940 Acc: 0.7897\n",
            "val Loss: 1.1795 Acc: 0.6424\n",
            "\n",
            "Epoch 81/99\n",
            "----------\n",
            "train Loss: 0.5846 Acc: 0.7917\n",
            "val Loss: 1.2019 Acc: 0.6315\n",
            "\n",
            "Epoch 82/99\n",
            "----------\n",
            "train Loss: 0.5761 Acc: 0.7962\n",
            "val Loss: 1.2213 Acc: 0.6305\n",
            "\n",
            "Epoch 83/99\n",
            "----------\n",
            "train Loss: 0.5696 Acc: 0.7964\n",
            "val Loss: 1.1833 Acc: 0.6389\n",
            "\n",
            "Epoch 84/99\n",
            "----------\n",
            "train Loss: 0.5659 Acc: 0.7998\n",
            "val Loss: 1.2498 Acc: 0.6308\n",
            "\n",
            "Epoch 85/99\n",
            "----------\n",
            "train Loss: 0.5639 Acc: 0.8003\n",
            "val Loss: 1.2023 Acc: 0.6404\n",
            "\n",
            "Epoch 86/99\n",
            "----------\n",
            "train Loss: 0.5553 Acc: 0.8016\n",
            "val Loss: 1.2081 Acc: 0.6414\n",
            "\n",
            "Epoch 87/99\n",
            "----------\n",
            "train Loss: 0.5500 Acc: 0.8031\n",
            "val Loss: 1.2217 Acc: 0.6378\n",
            "\n",
            "Epoch 88/99\n",
            "----------\n",
            "train Loss: 0.5422 Acc: 0.8078\n",
            "val Loss: 1.2245 Acc: 0.6420\n",
            "\n",
            "Epoch 89/99\n",
            "----------\n",
            "train Loss: 0.5377 Acc: 0.8063\n",
            "val Loss: 1.2414 Acc: 0.6396\n",
            "\n",
            "Epoch 90/99\n",
            "----------\n",
            "train Loss: 0.5295 Acc: 0.8107\n",
            "val Loss: 1.2799 Acc: 0.6399\n",
            "\n",
            "Epoch 91/99\n",
            "----------\n",
            "train Loss: 0.5253 Acc: 0.8125\n",
            "val Loss: 1.2304 Acc: 0.6372\n",
            "\n",
            "Epoch 92/99\n",
            "----------\n",
            "train Loss: 0.5245 Acc: 0.8107\n",
            "val Loss: 1.2523 Acc: 0.6368\n",
            "\n",
            "Epoch 93/99\n",
            "----------\n",
            "train Loss: 0.5134 Acc: 0.8180\n",
            "val Loss: 1.2705 Acc: 0.6377\n",
            "\n",
            "Epoch 94/99\n",
            "----------\n",
            "train Loss: 0.5067 Acc: 0.8184\n",
            "val Loss: 1.2781 Acc: 0.6421\n",
            "\n",
            "Epoch 95/99\n",
            "----------\n",
            "train Loss: 0.5008 Acc: 0.8209\n",
            "val Loss: 1.3097 Acc: 0.6381\n",
            "\n",
            "Epoch 96/99\n",
            "----------\n",
            "train Loss: 0.5022 Acc: 0.8196\n",
            "val Loss: 1.2905 Acc: 0.6401\n",
            "\n",
            "Epoch 97/99\n",
            "----------\n",
            "train Loss: 0.4956 Acc: 0.8205\n",
            "val Loss: 1.3446 Acc: 0.6303\n",
            "\n",
            "Epoch 98/99\n",
            "----------\n",
            "train Loss: 0.4924 Acc: 0.8236\n",
            "val Loss: 1.3158 Acc: 0.6347\n",
            "\n",
            "Epoch 99/99\n",
            "----------\n",
            "train Loss: 0.4879 Acc: 0.8253\n",
            "val Loss: 1.3289 Acc: 0.6301\n",
            "\n",
            "Training complete in 7m 8s\n",
            "Best val Acc: 0.652000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Net2(\n",
              "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=128, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_model(net2, criterion2, optimizer2,  num_epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wkqPo1GVl-Y"
      },
      "outputs": [],
      "source": [
        "class Net3(nn.Module):\n",
        "  def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.batchNorm1 = nn.BatchNorm2d(8)\n",
        "        self.conv2 = nn.Conv2d(8, 16, 5)\n",
        "        self.batchNorm2 = nn.BatchNorm2d(16)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.pool(F.relu(self.batchNorm1(self.conv1(x))))\n",
        "      x = self.pool(F.relu(self.batchNorm2(self.conv2(x))))\n",
        "      x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLsDhro6WQuV"
      },
      "outputs": [],
      "source": [
        "net3 = Net3()\n",
        "net3 = net3.to(device)\n",
        "criterion3 = nn.CrossEntropyLoss()\n",
        "optimizer3 = optim.SGD(net3.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZrZpSFDUWVPI",
        "outputId": "46fd4a49-1afc-4113-f163-478e95cabfc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 1.8349 Acc: 0.3371\n",
            "val Loss: 1.5999 Acc: 0.3897\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 1.4371 Acc: 0.4775\n",
            "val Loss: 1.5773 Acc: 0.4207\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 1.2916 Acc: 0.5369\n",
            "val Loss: 1.3068 Acc: 0.5357\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 1.1964 Acc: 0.5756\n",
            "val Loss: 1.1560 Acc: 0.5911\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 1.1258 Acc: 0.6016\n",
            "val Loss: 1.1732 Acc: 0.5815\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 1.0732 Acc: 0.6235\n",
            "val Loss: 1.0736 Acc: 0.6244\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 1.0262 Acc: 0.6375\n",
            "val Loss: 1.0768 Acc: 0.6192\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 0.9854 Acc: 0.6535\n",
            "val Loss: 1.0597 Acc: 0.6268\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 0.9458 Acc: 0.6698\n",
            "val Loss: 1.0279 Acc: 0.6326\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 0.9148 Acc: 0.6777\n",
            "val Loss: 1.0382 Acc: 0.6368\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 0.8842 Acc: 0.6913\n",
            "val Loss: 0.9533 Acc: 0.6658\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 0.8527 Acc: 0.7024\n",
            "val Loss: 0.9753 Acc: 0.6583\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 0.8308 Acc: 0.7078\n",
            "val Loss: 1.0317 Acc: 0.6394\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 0.8096 Acc: 0.7155\n",
            "val Loss: 1.0347 Acc: 0.6441\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 0.7835 Acc: 0.7238\n",
            "val Loss: 1.0049 Acc: 0.6593\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 0.7631 Acc: 0.7329\n",
            "val Loss: 1.0394 Acc: 0.6381\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 0.7454 Acc: 0.7375\n",
            "val Loss: 1.0541 Acc: 0.6384\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 0.7245 Acc: 0.7449\n",
            "val Loss: 0.8947 Acc: 0.6887\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 0.7026 Acc: 0.7523\n",
            "val Loss: 0.9239 Acc: 0.6870\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 0.6866 Acc: 0.7598\n",
            "val Loss: 0.9814 Acc: 0.6672\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 0.6689 Acc: 0.7624\n",
            "val Loss: 1.2272 Acc: 0.6099\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 0.6521 Acc: 0.7706\n",
            "val Loss: 0.9572 Acc: 0.6783\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 0.6364 Acc: 0.7742\n",
            "val Loss: 0.9649 Acc: 0.6772\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 0.6176 Acc: 0.7827\n",
            "val Loss: 1.0203 Acc: 0.6621\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 0.6006 Acc: 0.7865\n",
            "val Loss: 0.9903 Acc: 0.6749\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 0.5816 Acc: 0.7943\n",
            "val Loss: 1.0882 Acc: 0.6453\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 0.5715 Acc: 0.7963\n",
            "val Loss: 0.9843 Acc: 0.6822\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 0.5565 Acc: 0.8007\n",
            "val Loss: 0.9957 Acc: 0.6828\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 0.5397 Acc: 0.8099\n",
            "val Loss: 1.1236 Acc: 0.6450\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 0.5268 Acc: 0.8135\n",
            "val Loss: 0.9927 Acc: 0.6858\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 0.5121 Acc: 0.8172\n",
            "val Loss: 1.3607 Acc: 0.6074\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 0.5042 Acc: 0.8194\n",
            "val Loss: 1.0367 Acc: 0.6824\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 0.4826 Acc: 0.8295\n",
            "val Loss: 1.0995 Acc: 0.6691\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 0.4720 Acc: 0.8313\n",
            "val Loss: 1.1129 Acc: 0.6738\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 0.4611 Acc: 0.8347\n",
            "val Loss: 1.0632 Acc: 0.6781\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 0.4472 Acc: 0.8393\n",
            "val Loss: 1.1673 Acc: 0.6570\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 0.4354 Acc: 0.8441\n",
            "val Loss: 1.1763 Acc: 0.6632\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 0.4263 Acc: 0.8477\n",
            "val Loss: 1.1340 Acc: 0.6732\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 0.4171 Acc: 0.8509\n",
            "val Loss: 1.1761 Acc: 0.6712\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 0.4058 Acc: 0.8539\n",
            "val Loss: 1.2092 Acc: 0.6639\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 0.3935 Acc: 0.8615\n",
            "val Loss: 1.3782 Acc: 0.6328\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 0.3829 Acc: 0.8639\n",
            "val Loss: 1.2494 Acc: 0.6590\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 0.3735 Acc: 0.8670\n",
            "val Loss: 1.2295 Acc: 0.6706\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 0.3604 Acc: 0.8698\n",
            "val Loss: 1.2781 Acc: 0.6623\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 0.3559 Acc: 0.8718\n",
            "val Loss: 1.2610 Acc: 0.6700\n",
            "\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 0.3471 Acc: 0.8753\n",
            "val Loss: 1.2984 Acc: 0.6737\n",
            "\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 0.3352 Acc: 0.8781\n",
            "val Loss: 1.3310 Acc: 0.6610\n",
            "\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 0.3286 Acc: 0.8828\n",
            "val Loss: 1.3147 Acc: 0.6727\n",
            "\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 0.3156 Acc: 0.8865\n",
            "val Loss: 1.3378 Acc: 0.6681\n",
            "\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 0.3067 Acc: 0.8883\n",
            "val Loss: 1.4300 Acc: 0.6629\n",
            "\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 0.3021 Acc: 0.8917\n",
            "val Loss: 1.3662 Acc: 0.6718\n",
            "\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 0.2955 Acc: 0.8926\n",
            "val Loss: 1.4446 Acc: 0.6608\n",
            "\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 0.2859 Acc: 0.8982\n",
            "val Loss: 1.4962 Acc: 0.6556\n",
            "\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 0.2832 Acc: 0.8981\n",
            "val Loss: 1.6061 Acc: 0.6452\n",
            "\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 0.2738 Acc: 0.9028\n",
            "val Loss: 1.4864 Acc: 0.6634\n",
            "\n",
            "Epoch 55/99\n",
            "----------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-fb3f51375f10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer3\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-aa3894ee8b73>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-d12e43e7b427>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchNorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchNorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# flatten all dimensions except batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mbn_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mexponential_average_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         )\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2438\u001b[0m     return torch.batch_norm(\n\u001b[0;32m-> 2439\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2440\u001b[0m     )\n\u001b[1;32m   2441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_model(net3, criterion3, optimizer3,  num_epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5B0-6-FW_Ur"
      },
      "outputs": [],
      "source": [
        "net3 = Net3()\n",
        "net3 = net3.to(device)\n",
        "criterion3 = nn.CrossEntropyLoss()\n",
        "optimizer3 = optim.Adam(net3.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fnwJVknOXSvg",
        "outputId": "4747dd04-b494-4f17-986b-567e6eaff51a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 1.5068 Acc: 0.4494\n",
            "val Loss: 1.3117 Acc: 0.5207\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 1.2462 Acc: 0.5511\n",
            "val Loss: 1.2599 Acc: 0.5448\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 1.1287 Acc: 0.5970\n",
            "val Loss: 1.2411 Acc: 0.5597\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 1.0450 Acc: 0.6310\n",
            "val Loss: 1.1093 Acc: 0.6096\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 0.9828 Acc: 0.6523\n",
            "val Loss: 1.2477 Acc: 0.5624\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 0.9267 Acc: 0.6718\n",
            "val Loss: 1.0273 Acc: 0.6455\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 0.8814 Acc: 0.6874\n",
            "val Loss: 1.0595 Acc: 0.6309\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 0.8437 Acc: 0.7046\n",
            "val Loss: 1.1252 Acc: 0.6118\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 0.8083 Acc: 0.7144\n",
            "val Loss: 1.0643 Acc: 0.6314\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 0.7746 Acc: 0.7263\n",
            "val Loss: 1.1169 Acc: 0.6270\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 0.7460 Acc: 0.7371\n",
            "val Loss: 1.0152 Acc: 0.6530\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 0.7135 Acc: 0.7484\n",
            "val Loss: 1.0416 Acc: 0.6539\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 0.6891 Acc: 0.7567\n",
            "val Loss: 1.2454 Acc: 0.6102\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 0.6671 Acc: 0.7636\n",
            "val Loss: 1.0950 Acc: 0.6465\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 0.6432 Acc: 0.7717\n",
            "val Loss: 1.0729 Acc: 0.6550\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 0.6220 Acc: 0.7793\n",
            "val Loss: 1.1103 Acc: 0.6485\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 0.5982 Acc: 0.7864\n",
            "val Loss: 1.1007 Acc: 0.6573\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 0.5801 Acc: 0.7933\n",
            "val Loss: 1.1485 Acc: 0.6493\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 0.5622 Acc: 0.7994\n",
            "val Loss: 1.2035 Acc: 0.6421\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 0.5411 Acc: 0.8070\n",
            "val Loss: 1.3030 Acc: 0.6174\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 0.5211 Acc: 0.8136\n",
            "val Loss: 1.1845 Acc: 0.6495\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 0.5123 Acc: 0.8166\n",
            "val Loss: 1.2093 Acc: 0.6498\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 0.4951 Acc: 0.8213\n",
            "val Loss: 1.2710 Acc: 0.6372\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 0.4761 Acc: 0.8296\n",
            "val Loss: 1.3269 Acc: 0.6353\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 0.4648 Acc: 0.8328\n",
            "val Loss: 1.3729 Acc: 0.6252\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 0.4508 Acc: 0.8371\n",
            "val Loss: 1.4224 Acc: 0.6348\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 0.4435 Acc: 0.8410\n",
            "val Loss: 1.3823 Acc: 0.6375\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 0.4236 Acc: 0.8478\n",
            "val Loss: 1.3466 Acc: 0.6489\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 0.4146 Acc: 0.8518\n",
            "val Loss: 1.4340 Acc: 0.6334\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 0.4044 Acc: 0.8534\n",
            "val Loss: 1.4675 Acc: 0.6383\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-fb3f51375f10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer3\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-aa3894ee8b73>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    169\u001b[0m                  \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                  \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'foreach'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                  capturable=group['capturable'])\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    224\u001b[0m          \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m          \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m          capturable=capturable)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_model(net3, criterion3, optimizer3,  num_epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDmISpuOYC2b"
      },
      "outputs": [],
      "source": [
        "class Net4(nn.Module):\n",
        "  def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.batchNorm1 = nn.BatchNorm2d(8)\n",
        "        self.conv2 = nn.Conv2d(8, 16, 5)\n",
        "        self.batchNorm2 = nn.BatchNorm2d(16)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.pool(F.relu(self.batchNorm1(self.conv1(x))))\n",
        "      x = self.pool(F.relu(self.batchNorm2(self.conv2(x))))\n",
        "      x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "      x = F.sigmoid(self.fc1(x))\n",
        "      x = F.sigmoid(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTiT7sI1YJc_"
      },
      "outputs": [],
      "source": [
        "net4 = Net4()\n",
        "net4 = net4.to(device)\n",
        "criterion4 = nn.CrossEntropyLoss()\n",
        "optimizer4 = optim.SGD(net4.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOOkZpcjYPkX",
        "outputId": "a4f6c99c-739f-49c9-93f3-3f4e399d9128"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 2.2961 Acc: 0.1266\n",
            "val Loss: 2.2753 Acc: 0.1840\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 2.1950 Acc: 0.1851\n",
            "val Loss: 2.0926 Acc: 0.1850\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 1.9673 Acc: 0.2041\n",
            "val Loss: 1.9159 Acc: 0.2104\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 1.9019 Acc: 0.2184\n",
            "val Loss: 1.9665 Acc: 0.2160\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 1.8767 Acc: 0.2434\n",
            "val Loss: 1.9700 Acc: 0.2194\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 1.8472 Acc: 0.2750\n",
            "val Loss: 1.8949 Acc: 0.2540\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 1.7912 Acc: 0.3046\n",
            "val Loss: 1.7499 Acc: 0.3053\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 1.7206 Acc: 0.3322\n",
            "val Loss: 1.7232 Acc: 0.3107\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 1.6667 Acc: 0.3569\n",
            "val Loss: 1.8512 Acc: 0.2951\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 1.6198 Acc: 0.3805\n",
            "val Loss: 1.7887 Acc: 0.2950\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 1.5782 Acc: 0.4048\n",
            "val Loss: 1.5496 Acc: 0.4287\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 1.5281 Acc: 0.4290\n",
            "val Loss: 1.4828 Acc: 0.4483\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 1.4695 Acc: 0.4514\n",
            "val Loss: 1.4366 Acc: 0.4637\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 1.4124 Acc: 0.4744\n",
            "val Loss: 1.3948 Acc: 0.4832\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 1.3649 Acc: 0.4947\n",
            "val Loss: 1.5164 Acc: 0.4347\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 1.3152 Acc: 0.5130\n",
            "val Loss: 1.5824 Acc: 0.4525\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 1.2812 Acc: 0.5271\n",
            "val Loss: 1.2606 Acc: 0.5334\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 1.2453 Acc: 0.5444\n",
            "val Loss: 1.2939 Acc: 0.5315\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 1.2206 Acc: 0.5537\n",
            "val Loss: 1.2415 Acc: 0.5468\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 1.1880 Acc: 0.5676\n",
            "val Loss: 1.4171 Acc: 0.4885\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 1.1628 Acc: 0.5794\n",
            "val Loss: 1.2171 Acc: 0.5559\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 1.1374 Acc: 0.5883\n",
            "val Loss: 1.3659 Acc: 0.5038\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 1.1165 Acc: 0.5974\n",
            "val Loss: 1.3581 Acc: 0.5464\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 1.0997 Acc: 0.6044\n",
            "val Loss: 1.2429 Acc: 0.5498\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 1.0800 Acc: 0.6102\n",
            "val Loss: 1.1820 Acc: 0.5680\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 1.0699 Acc: 0.6165\n",
            "val Loss: 1.0946 Acc: 0.6095\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 1.0529 Acc: 0.6237\n",
            "val Loss: 1.2094 Acc: 0.5728\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 1.0379 Acc: 0.6287\n",
            "val Loss: 1.1075 Acc: 0.6005\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 1.0273 Acc: 0.6337\n",
            "val Loss: 1.4179 Acc: 0.5007\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 1.0154 Acc: 0.6362\n",
            "val Loss: 1.1476 Acc: 0.6037\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 1.0017 Acc: 0.6418\n",
            "val Loss: 1.1316 Acc: 0.6059\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 0.9898 Acc: 0.6470\n",
            "val Loss: 1.1558 Acc: 0.5958\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 0.9794 Acc: 0.6496\n",
            "val Loss: 1.1502 Acc: 0.5944\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 0.9668 Acc: 0.6573\n",
            "val Loss: 1.0802 Acc: 0.6205\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 0.9573 Acc: 0.6566\n",
            "val Loss: 1.0469 Acc: 0.6349\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 0.9471 Acc: 0.6640\n",
            "val Loss: 1.0176 Acc: 0.6462\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 0.9371 Acc: 0.6675\n",
            "val Loss: 1.0894 Acc: 0.6084\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 0.9285 Acc: 0.6697\n",
            "val Loss: 1.0683 Acc: 0.6191\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 0.9141 Acc: 0.6754\n",
            "val Loss: 1.0385 Acc: 0.6336\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 0.9058 Acc: 0.6780\n",
            "val Loss: 0.9972 Acc: 0.6463\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 0.8976 Acc: 0.6814\n",
            "val Loss: 1.0128 Acc: 0.6447\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 0.8917 Acc: 0.6821\n",
            "val Loss: 1.1077 Acc: 0.6217\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 0.8784 Acc: 0.6904\n",
            "val Loss: 1.0205 Acc: 0.6330\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 0.8731 Acc: 0.6883\n",
            "val Loss: 1.1998 Acc: 0.5815\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 0.8648 Acc: 0.6901\n",
            "val Loss: 1.0605 Acc: 0.6307\n",
            "\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 0.8528 Acc: 0.6974\n",
            "val Loss: 0.9505 Acc: 0.6689\n",
            "\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 0.8498 Acc: 0.6984\n",
            "val Loss: 0.9934 Acc: 0.6503\n",
            "\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 0.8426 Acc: 0.7005\n",
            "val Loss: 1.0180 Acc: 0.6447\n",
            "\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 0.8299 Acc: 0.7074\n",
            "val Loss: 1.0193 Acc: 0.6391\n",
            "\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 0.8238 Acc: 0.7083\n",
            "val Loss: 0.9722 Acc: 0.6533\n",
            "\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 0.8152 Acc: 0.7121\n",
            "val Loss: 0.9516 Acc: 0.6726\n",
            "\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 0.8067 Acc: 0.7148\n",
            "val Loss: 0.9696 Acc: 0.6575\n",
            "\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 0.8020 Acc: 0.7176\n",
            "val Loss: 1.0827 Acc: 0.6302\n",
            "\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 0.7926 Acc: 0.7186\n",
            "val Loss: 0.9735 Acc: 0.6614\n",
            "\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 0.7848 Acc: 0.7256\n",
            "val Loss: 0.9915 Acc: 0.6547\n",
            "\n",
            "Epoch 55/99\n",
            "----------\n",
            "train Loss: 0.7807 Acc: 0.7244\n",
            "val Loss: 0.9406 Acc: 0.6699\n",
            "\n",
            "Epoch 56/99\n",
            "----------\n",
            "train Loss: 0.7751 Acc: 0.7267\n",
            "val Loss: 0.9987 Acc: 0.6601\n",
            "\n",
            "Epoch 57/99\n",
            "----------\n",
            "train Loss: 0.7614 Acc: 0.7317\n",
            "val Loss: 1.0283 Acc: 0.6437\n",
            "\n",
            "Epoch 58/99\n",
            "----------\n",
            "train Loss: 0.7545 Acc: 0.7353\n",
            "val Loss: 0.9407 Acc: 0.6733\n",
            "\n",
            "Epoch 59/99\n",
            "----------\n",
            "train Loss: 0.7484 Acc: 0.7370\n",
            "val Loss: 0.9579 Acc: 0.6691\n",
            "\n",
            "Epoch 60/99\n",
            "----------\n",
            "train Loss: 0.7403 Acc: 0.7387\n",
            "val Loss: 0.9288 Acc: 0.6748\n",
            "\n",
            "Epoch 61/99\n",
            "----------\n",
            "train Loss: 0.7322 Acc: 0.7418\n",
            "val Loss: 0.9523 Acc: 0.6693\n",
            "\n",
            "Epoch 62/99\n",
            "----------\n",
            "train Loss: 0.7265 Acc: 0.7435\n",
            "val Loss: 0.9795 Acc: 0.6636\n",
            "\n",
            "Epoch 63/99\n",
            "----------\n",
            "train Loss: 0.7201 Acc: 0.7450\n",
            "val Loss: 0.9454 Acc: 0.6731\n",
            "\n",
            "Epoch 64/99\n",
            "----------\n",
            "train Loss: 0.7146 Acc: 0.7478\n",
            "val Loss: 1.0416 Acc: 0.6422\n",
            "\n",
            "Epoch 65/99\n",
            "----------\n",
            "train Loss: 0.7031 Acc: 0.7510\n",
            "val Loss: 0.9456 Acc: 0.6778\n",
            "\n",
            "Epoch 66/99\n",
            "----------\n",
            "train Loss: 0.6978 Acc: 0.7540\n",
            "val Loss: 0.9105 Acc: 0.6834\n",
            "\n",
            "Epoch 67/99\n",
            "----------\n",
            "train Loss: 0.6902 Acc: 0.7561\n",
            "val Loss: 0.9219 Acc: 0.6785\n",
            "\n",
            "Epoch 68/99\n",
            "----------\n",
            "train Loss: 0.6813 Acc: 0.7591\n",
            "val Loss: 0.9661 Acc: 0.6690\n",
            "\n",
            "Epoch 69/99\n",
            "----------\n",
            "train Loss: 0.6751 Acc: 0.7618\n",
            "val Loss: 0.9321 Acc: 0.6794\n",
            "\n",
            "Epoch 70/99\n",
            "----------\n",
            "train Loss: 0.6689 Acc: 0.7650\n",
            "val Loss: 0.9164 Acc: 0.6838\n",
            "\n",
            "Epoch 71/99\n",
            "----------\n",
            "train Loss: 0.6617 Acc: 0.7671\n",
            "val Loss: 0.9287 Acc: 0.6818\n",
            "\n",
            "Epoch 72/99\n",
            "----------\n",
            "train Loss: 0.6512 Acc: 0.7721\n",
            "val Loss: 0.9587 Acc: 0.6732\n",
            "\n",
            "Epoch 73/99\n",
            "----------\n",
            "train Loss: 0.6448 Acc: 0.7745\n",
            "val Loss: 0.9067 Acc: 0.6891\n",
            "\n",
            "Epoch 74/99\n",
            "----------\n",
            "train Loss: 0.6391 Acc: 0.7766\n",
            "val Loss: 0.9479 Acc: 0.6787\n",
            "\n",
            "Epoch 75/99\n",
            "----------\n",
            "train Loss: 0.6329 Acc: 0.7791\n",
            "val Loss: 0.9044 Acc: 0.6914\n",
            "\n",
            "Epoch 76/99\n",
            "----------\n",
            "train Loss: 0.6258 Acc: 0.7819\n",
            "val Loss: 0.9808 Acc: 0.6658\n",
            "\n",
            "Epoch 77/99\n",
            "----------\n",
            "train Loss: 0.6181 Acc: 0.7824\n",
            "val Loss: 1.0842 Acc: 0.6364\n",
            "\n",
            "Epoch 78/99\n",
            "----------\n",
            "train Loss: 0.6117 Acc: 0.7849\n",
            "val Loss: 0.9106 Acc: 0.6866\n",
            "\n",
            "Epoch 79/99\n",
            "----------\n",
            "train Loss: 0.6041 Acc: 0.7875\n",
            "val Loss: 0.9397 Acc: 0.6800\n",
            "\n",
            "Epoch 80/99\n",
            "----------\n",
            "train Loss: 0.5955 Acc: 0.7920\n",
            "val Loss: 1.0033 Acc: 0.6637\n",
            "\n",
            "Epoch 81/99\n",
            "----------\n",
            "train Loss: 0.5869 Acc: 0.7947\n",
            "val Loss: 0.9459 Acc: 0.6791\n",
            "\n",
            "Epoch 82/99\n",
            "----------\n",
            "train Loss: 0.5811 Acc: 0.7974\n",
            "val Loss: 0.9771 Acc: 0.6726\n",
            "\n",
            "Epoch 83/99\n",
            "----------\n",
            "train Loss: 0.5756 Acc: 0.8004\n",
            "val Loss: 0.9347 Acc: 0.6836\n",
            "\n",
            "Epoch 84/99\n",
            "----------\n",
            "train Loss: 0.5678 Acc: 0.8021\n",
            "val Loss: 1.0127 Acc: 0.6649\n",
            "\n",
            "Epoch 85/99\n",
            "----------\n",
            "train Loss: 0.5594 Acc: 0.8061\n",
            "val Loss: 0.9663 Acc: 0.6766\n",
            "\n",
            "Epoch 86/99\n",
            "----------\n",
            "train Loss: 0.5549 Acc: 0.8081\n",
            "val Loss: 0.9422 Acc: 0.6853\n",
            "\n",
            "Epoch 87/99\n",
            "----------\n",
            "train Loss: 0.5472 Acc: 0.8083\n",
            "val Loss: 0.9221 Acc: 0.6892\n",
            "\n",
            "Epoch 88/99\n",
            "----------\n",
            "train Loss: 0.5413 Acc: 0.8133\n",
            "val Loss: 0.9432 Acc: 0.6801\n",
            "\n",
            "Epoch 89/99\n",
            "----------\n",
            "train Loss: 0.5337 Acc: 0.8137\n",
            "val Loss: 1.0568 Acc: 0.6600\n",
            "\n",
            "Epoch 90/99\n",
            "----------\n",
            "train Loss: 0.5274 Acc: 0.8181\n",
            "val Loss: 0.9509 Acc: 0.6806\n",
            "\n",
            "Epoch 91/99\n",
            "----------\n",
            "train Loss: 0.5215 Acc: 0.8199\n",
            "val Loss: 0.9704 Acc: 0.6806\n",
            "\n",
            "Epoch 92/99\n",
            "----------\n",
            "train Loss: 0.5112 Acc: 0.8211\n",
            "val Loss: 0.9722 Acc: 0.6803\n",
            "\n",
            "Epoch 93/99\n",
            "----------\n",
            "train Loss: 0.5112 Acc: 0.8208\n",
            "val Loss: 0.9865 Acc: 0.6786\n",
            "\n",
            "Epoch 94/99\n",
            "----------\n",
            "train Loss: 0.5003 Acc: 0.8276\n",
            "val Loss: 0.9689 Acc: 0.6831\n",
            "\n",
            "Epoch 95/99\n",
            "----------\n",
            "train Loss: 0.4926 Acc: 0.8297\n",
            "val Loss: 0.9668 Acc: 0.6826\n",
            "\n",
            "Epoch 96/99\n",
            "----------\n",
            "train Loss: 0.4874 Acc: 0.8305\n",
            "val Loss: 1.0190 Acc: 0.6681\n",
            "\n",
            "Epoch 97/99\n",
            "----------\n",
            "train Loss: 0.4796 Acc: 0.8347\n",
            "val Loss: 0.9768 Acc: 0.6787\n",
            "\n",
            "Epoch 98/99\n",
            "----------\n",
            "train Loss: 0.4762 Acc: 0.8346\n",
            "val Loss: 0.9925 Acc: 0.6760\n",
            "\n",
            "Epoch 99/99\n",
            "----------\n",
            "train Loss: 0.4663 Acc: 0.8397\n",
            "val Loss: 1.0257 Acc: 0.6708\n",
            "\n",
            "Training complete in 6m 58s\n",
            "Best val Acc: 0.691400\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Net4(\n",
              "  (conv1): Conv2d(3, 8, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (batchNorm1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (batchNorm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_model(net4, criterion4, optimizer4,  num_epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlJwYRFjjBsw",
        "outputId": "2933e5e1-fd2f-4351-b157-4155f89ba550"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "resnet = models.resnet18(pretrained=False)\n",
        "num_ftrs = resnet.fc.in_features\n",
        "resnet.fc = nn.Linear(num_ftrs, 10)\n",
        "resnet.to(device)\n",
        "criterion_res = nn.CrossEntropyLoss()\n",
        "optimizer_res = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54fMHuWejbhH",
        "outputId": "6d496930-b744-430a-d5dc-ebc7fa594f9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 1.6228 Acc: 0.4115\n",
            "val Loss: 1.4019 Acc: 0.4893\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 1.2891 Acc: 0.5380\n",
            "val Loss: 1.1808 Acc: 0.5728\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 1.0991 Acc: 0.6071\n",
            "val Loss: 1.1357 Acc: 0.5982\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 0.9539 Acc: 0.6640\n",
            "val Loss: 1.1329 Acc: 0.6058\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 0.8324 Acc: 0.7056\n",
            "val Loss: 1.0408 Acc: 0.6425\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 0.7159 Acc: 0.7472\n",
            "val Loss: 1.1156 Acc: 0.6241\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 0.6186 Acc: 0.7799\n",
            "val Loss: 1.1240 Acc: 0.6359\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 0.5278 Acc: 0.8131\n",
            "val Loss: 1.1306 Acc: 0.6428\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 0.4575 Acc: 0.8356\n",
            "val Loss: 1.1255 Acc: 0.6596\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 0.3754 Acc: 0.8657\n",
            "val Loss: 1.2724 Acc: 0.6505\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 0.3154 Acc: 0.8879\n",
            "val Loss: 1.3206 Acc: 0.6433\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 0.2729 Acc: 0.9022\n",
            "val Loss: 1.3080 Acc: 0.6572\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 0.2330 Acc: 0.9184\n",
            "val Loss: 1.3142 Acc: 0.6703\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 0.1924 Acc: 0.9325\n",
            "val Loss: 1.4050 Acc: 0.6597\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 0.1644 Acc: 0.9425\n",
            "val Loss: 1.3983 Acc: 0.6629\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 0.1424 Acc: 0.9510\n",
            "val Loss: 1.4632 Acc: 0.6659\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 0.1382 Acc: 0.9510\n",
            "val Loss: 1.4668 Acc: 0.6656\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 0.1285 Acc: 0.9556\n",
            "val Loss: 1.5116 Acc: 0.6725\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 0.1084 Acc: 0.9623\n",
            "val Loss: 1.5420 Acc: 0.6847\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 0.0932 Acc: 0.9679\n",
            "val Loss: 1.6010 Acc: 0.6805\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 0.0806 Acc: 0.9719\n",
            "val Loss: 1.7212 Acc: 0.6680\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 0.0818 Acc: 0.9716\n",
            "val Loss: 1.6095 Acc: 0.6820\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 0.0705 Acc: 0.9759\n",
            "val Loss: 1.5804 Acc: 0.6899\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 0.0602 Acc: 0.9793\n",
            "val Loss: 1.6056 Acc: 0.6834\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 0.0576 Acc: 0.9804\n",
            "val Loss: 1.5894 Acc: 0.6966\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 0.0566 Acc: 0.9802\n",
            "val Loss: 1.6339 Acc: 0.6944\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 0.0539 Acc: 0.9807\n",
            "val Loss: 1.6642 Acc: 0.6900\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 0.0523 Acc: 0.9817\n",
            "val Loss: 1.6917 Acc: 0.6916\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 0.0438 Acc: 0.9849\n",
            "val Loss: 1.7335 Acc: 0.6857\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 0.0380 Acc: 0.9879\n",
            "val Loss: 1.7138 Acc: 0.6955\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 0.0399 Acc: 0.9864\n",
            "val Loss: 1.6792 Acc: 0.6949\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 0.0330 Acc: 0.9892\n",
            "val Loss: 1.6930 Acc: 0.6966\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 0.0317 Acc: 0.9891\n",
            "val Loss: 1.7387 Acc: 0.6929\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 0.0350 Acc: 0.9878\n",
            "val Loss: 1.7595 Acc: 0.6992\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 0.0284 Acc: 0.9907\n",
            "val Loss: 1.7424 Acc: 0.7003\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 0.0251 Acc: 0.9920\n",
            "val Loss: 1.7646 Acc: 0.6972\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 0.0193 Acc: 0.9936\n",
            "val Loss: 1.8089 Acc: 0.7011\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 0.0237 Acc: 0.9918\n",
            "val Loss: 1.8724 Acc: 0.6949\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 0.0286 Acc: 0.9901\n",
            "val Loss: 1.8612 Acc: 0.6955\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 0.0256 Acc: 0.9917\n",
            "val Loss: 1.8197 Acc: 0.6975\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 0.0260 Acc: 0.9911\n",
            "val Loss: 1.8189 Acc: 0.6972\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 0.0256 Acc: 0.9918\n",
            "val Loss: 1.8081 Acc: 0.6995\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 0.0236 Acc: 0.9921\n",
            "val Loss: 1.8501 Acc: 0.6961\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 0.0262 Acc: 0.9906\n",
            "val Loss: 1.8891 Acc: 0.6937\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 0.0230 Acc: 0.9922\n",
            "val Loss: 1.7784 Acc: 0.7076\n",
            "\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 0.0163 Acc: 0.9947\n",
            "val Loss: 1.8337 Acc: 0.7028\n",
            "\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 0.0150 Acc: 0.9949\n",
            "val Loss: 1.8412 Acc: 0.6980\n",
            "\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 0.0162 Acc: 0.9949\n",
            "val Loss: 1.8532 Acc: 0.7041\n",
            "\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 0.0197 Acc: 0.9937\n",
            "val Loss: 1.8855 Acc: 0.7006\n",
            "\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 0.0168 Acc: 0.9948\n",
            "val Loss: 1.8746 Acc: 0.7066\n",
            "\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 0.0116 Acc: 0.9963\n",
            "val Loss: 1.9368 Acc: 0.6994\n",
            "\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 0.0114 Acc: 0.9965\n",
            "val Loss: 1.9421 Acc: 0.7060\n",
            "\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 0.0135 Acc: 0.9956\n",
            "val Loss: 1.9398 Acc: 0.6973\n",
            "\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 0.0109 Acc: 0.9962\n",
            "val Loss: 1.9445 Acc: 0.7022\n",
            "\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 0.0127 Acc: 0.9959\n",
            "val Loss: 1.9337 Acc: 0.7058\n",
            "\n",
            "Epoch 55/99\n",
            "----------\n",
            "train Loss: 0.0106 Acc: 0.9966\n",
            "val Loss: 1.8919 Acc: 0.7068\n",
            "\n",
            "Epoch 56/99\n",
            "----------\n",
            "train Loss: 0.0094 Acc: 0.9972\n",
            "val Loss: 1.9000 Acc: 0.7071\n",
            "\n",
            "Epoch 57/99\n",
            "----------\n",
            "train Loss: 0.0078 Acc: 0.9975\n",
            "val Loss: 1.9424 Acc: 0.7035\n",
            "\n",
            "Epoch 58/99\n",
            "----------\n",
            "train Loss: 0.0078 Acc: 0.9977\n",
            "val Loss: 1.9061 Acc: 0.7064\n",
            "\n",
            "Epoch 59/99\n",
            "----------\n",
            "train Loss: 0.0091 Acc: 0.9965\n",
            "val Loss: 1.9996 Acc: 0.6955\n",
            "\n",
            "Epoch 60/99\n",
            "----------\n",
            "train Loss: 0.0080 Acc: 0.9974\n",
            "val Loss: 1.9613 Acc: 0.7082\n",
            "\n",
            "Epoch 61/99\n",
            "----------\n",
            "train Loss: 0.0086 Acc: 0.9973\n",
            "val Loss: 1.9806 Acc: 0.7028\n",
            "\n",
            "Epoch 62/99\n",
            "----------\n",
            "train Loss: 0.0085 Acc: 0.9972\n",
            "val Loss: 1.9641 Acc: 0.7082\n",
            "\n",
            "Epoch 63/99\n",
            "----------\n",
            "train Loss: 0.0068 Acc: 0.9982\n",
            "val Loss: 1.9481 Acc: 0.7060\n",
            "\n",
            "Epoch 64/99\n",
            "----------\n",
            "train Loss: 0.0072 Acc: 0.9980\n",
            "val Loss: 2.0162 Acc: 0.7011\n",
            "\n",
            "Epoch 65/99\n",
            "----------\n",
            "train Loss: 0.0084 Acc: 0.9974\n",
            "val Loss: 2.0013 Acc: 0.7042\n",
            "\n",
            "Epoch 66/99\n",
            "----------\n",
            "train Loss: 0.0093 Acc: 0.9970\n",
            "val Loss: 1.9774 Acc: 0.7046\n",
            "\n",
            "Epoch 67/99\n",
            "----------\n",
            "train Loss: 0.0052 Acc: 0.9984\n",
            "val Loss: 1.9917 Acc: 0.7039\n",
            "\n",
            "Epoch 68/99\n",
            "----------\n",
            "train Loss: 0.0091 Acc: 0.9972\n",
            "val Loss: 1.9997 Acc: 0.7047\n",
            "\n",
            "Epoch 69/99\n",
            "----------\n",
            "train Loss: 0.0092 Acc: 0.9971\n",
            "val Loss: 1.9843 Acc: 0.7121\n",
            "\n",
            "Epoch 70/99\n",
            "----------\n",
            "train Loss: 0.0093 Acc: 0.9968\n",
            "val Loss: 2.0167 Acc: 0.7069\n",
            "\n",
            "Epoch 71/99\n",
            "----------\n",
            "train Loss: 0.0086 Acc: 0.9971\n",
            "val Loss: 2.0527 Acc: 0.7013\n",
            "\n",
            "Epoch 72/99\n",
            "----------\n",
            "train Loss: 0.0088 Acc: 0.9972\n",
            "val Loss: 2.0066 Acc: 0.7085\n",
            "\n",
            "Epoch 73/99\n",
            "----------\n",
            "train Loss: 0.0103 Acc: 0.9966\n",
            "val Loss: 2.0170 Acc: 0.7045\n",
            "\n",
            "Epoch 74/99\n",
            "----------\n",
            "train Loss: 0.0103 Acc: 0.9963\n",
            "val Loss: 2.0279 Acc: 0.7089\n",
            "\n",
            "Epoch 75/99\n",
            "----------\n",
            "train Loss: 0.0123 Acc: 0.9955\n",
            "val Loss: 2.0281 Acc: 0.7017\n",
            "\n",
            "Epoch 76/99\n",
            "----------\n",
            "train Loss: 0.0107 Acc: 0.9966\n",
            "val Loss: 2.0350 Acc: 0.7073\n",
            "\n",
            "Epoch 77/99\n",
            "----------\n",
            "train Loss: 0.0096 Acc: 0.9965\n",
            "val Loss: 2.0137 Acc: 0.7079\n",
            "\n",
            "Epoch 78/99\n",
            "----------\n",
            "train Loss: 0.0094 Acc: 0.9970\n",
            "val Loss: 2.0778 Acc: 0.7063\n",
            "\n",
            "Epoch 79/99\n",
            "----------\n",
            "train Loss: 0.0078 Acc: 0.9979\n",
            "val Loss: 2.0200 Acc: 0.7050\n",
            "\n",
            "Epoch 80/99\n",
            "----------\n",
            "train Loss: 0.0095 Acc: 0.9966\n",
            "val Loss: 2.0762 Acc: 0.7027\n",
            "\n",
            "Epoch 81/99\n",
            "----------\n",
            "train Loss: 0.0100 Acc: 0.9968\n",
            "val Loss: 2.0001 Acc: 0.7112\n",
            "\n",
            "Epoch 82/99\n",
            "----------\n",
            "train Loss: 0.0084 Acc: 0.9974\n",
            "val Loss: 2.0772 Acc: 0.7038\n",
            "\n",
            "Epoch 83/99\n",
            "----------\n",
            "train Loss: 0.0067 Acc: 0.9977\n",
            "val Loss: 2.0498 Acc: 0.7101\n",
            "\n",
            "Epoch 84/99\n",
            "----------\n",
            "train Loss: 0.0090 Acc: 0.9967\n",
            "val Loss: 2.0644 Acc: 0.7029\n",
            "\n",
            "Epoch 85/99\n",
            "----------\n",
            "train Loss: 0.0068 Acc: 0.9980\n",
            "val Loss: 2.0479 Acc: 0.7068\n",
            "\n",
            "Epoch 86/99\n",
            "----------\n",
            "train Loss: 0.0062 Acc: 0.9980\n",
            "val Loss: 2.0617 Acc: 0.7101\n",
            "\n",
            "Epoch 87/99\n",
            "----------\n",
            "train Loss: 0.0055 Acc: 0.9983\n",
            "val Loss: 2.0688 Acc: 0.7135\n",
            "\n",
            "Epoch 88/99\n",
            "----------\n",
            "train Loss: 0.0046 Acc: 0.9985\n",
            "val Loss: 2.0413 Acc: 0.7107\n",
            "\n",
            "Epoch 89/99\n",
            "----------\n",
            "train Loss: 0.0042 Acc: 0.9987\n",
            "val Loss: 2.0479 Acc: 0.7105\n",
            "\n",
            "Epoch 90/99\n",
            "----------\n",
            "train Loss: 0.0048 Acc: 0.9985\n",
            "val Loss: 2.0730 Acc: 0.7059\n",
            "\n",
            "Epoch 91/99\n",
            "----------\n",
            "train Loss: 0.0050 Acc: 0.9985\n",
            "val Loss: 2.0951 Acc: 0.7073\n",
            "\n",
            "Epoch 92/99\n",
            "----------\n",
            "train Loss: 0.0041 Acc: 0.9988\n",
            "val Loss: 2.0598 Acc: 0.7100\n",
            "\n",
            "Epoch 93/99\n",
            "----------\n",
            "train Loss: 0.0036 Acc: 0.9989\n",
            "val Loss: 2.0763 Acc: 0.7059\n",
            "\n",
            "Epoch 94/99\n",
            "----------\n",
            "train Loss: 0.0037 Acc: 0.9987\n",
            "val Loss: 2.1007 Acc: 0.7075\n",
            "\n",
            "Epoch 95/99\n",
            "----------\n",
            "train Loss: 0.0032 Acc: 0.9991\n",
            "val Loss: 2.1180 Acc: 0.7060\n",
            "\n",
            "Epoch 96/99\n",
            "----------\n",
            "train Loss: 0.0050 Acc: 0.9983\n",
            "val Loss: 2.1275 Acc: 0.7063\n",
            "\n",
            "Epoch 97/99\n",
            "----------\n",
            "train Loss: 0.0053 Acc: 0.9982\n",
            "val Loss: 2.0539 Acc: 0.7080\n",
            "\n",
            "Epoch 98/99\n",
            "----------\n",
            "train Loss: 0.0055 Acc: 0.9983\n",
            "val Loss: 2.1076 Acc: 0.7071\n",
            "\n",
            "Epoch 99/99\n",
            "----------\n",
            "train Loss: 0.0050 Acc: 0.9985\n",
            "val Loss: 2.0875 Acc: 0.7057\n",
            "\n",
            "Training complete in 31m 38s\n",
            "Best val Acc: 0.713500\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_model(resnet, criterion_res, optimizer_res,  num_epochs=n_epochs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
